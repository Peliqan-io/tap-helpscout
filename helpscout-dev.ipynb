{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import backoff\n",
    "import requests\n",
    "import pendulum\n",
    "import singer\n",
    "from singer import Transformer, utils\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = singer.get_logger()\n",
    "SESSION = requests.Session()\n",
    "REQUIRED_CONFIG_KEYS = [\n",
    "    \"start_date\",\n",
    "    \"client_id\",\n",
    "    \"client_secret\",\n",
    "    \"user_agent\",\n",
    "]\n",
    "BASE_API_URL = \"https://api.helpscout.net/v2/\"\n",
    "CONFIG = {}\n",
    "STATE = {}\n",
    "AUTH = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: Get working dirctories\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "print(sys.argv[0])\n",
    "print(os.path.dirname(os.path.realpath('__file__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: Set working directory and load config file from working dir (where config/state file are)\n",
    "os.chdir('C:/Files/Work/Clients/Stitch/tap-helpscout')\n",
    "my_config_path = 'tap_config.json'\n",
    "with open(my_config_path) as file:\n",
    "    CONFIG = json.load(file)\n",
    "\n",
    "my_state_path = 'state.json'\n",
    "with open(my_state_path) as file:\n",
    "    STATE = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_token_request():\n",
    "    response = requests.post(url=BASE_API_URL + 'oauth2/token',\n",
    "                             data={'client_id': CONFIG['client_id'],\n",
    "                                   'client_secret': CONFIG['client_secret'],\n",
    "                                   'grant_type': 'client_credentials'},\n",
    "                             headers={\"User-Agent\": CONFIG['user_agent']})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _refresh_access_token():\n",
    "    LOGGER.info(\"Refreshing access token\")\n",
    "    resp = _make_token_request()\n",
    "    resp_json = {}\n",
    "    try:\n",
    "        resp_json = resp.json()\n",
    "        expires_in_seconds = resp_json['expires_in'] - 120  # pad by 120 seconds\n",
    "        _expires_at = pendulum.now().add(seconds=expires_in_seconds)\n",
    "        _access_token = resp_json['access_token']\n",
    "    except KeyError as key_err:\n",
    "        if resp_json.get('error'):\n",
    "            LOGGER.critical(resp_json.get('error'))\n",
    "        if resp_json.get('error_description'):\n",
    "            LOGGER.critical(resp_json.get('error_description'))\n",
    "        raise key_err\n",
    "    LOGGER.info(\"Got refreshed access token\")\n",
    "    return _access_token, _expires_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(_access_token=None, _expires_at=None):\n",
    "    if _access_token is not None and _expires_at is not None:\n",
    "        if _expires_at > pendulum.now():\n",
    "            return _access_token, _expires_at\n",
    "    return _refresh_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH['access_token'], AUTH['expires_at'] = get_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(url, params=None):\n",
    "    params = params or {}\n",
    "    AUTH['access_token'], AUTH['expires_at'] = get_access_token(AUTH['access_token'], \\\n",
    "        AUTH['expires_at'])\n",
    "    headers = {\"Accept\": \"application/json\",\n",
    "               \"Authorization\": \"Bearer \" + AUTH['access_token'],\n",
    "               \"User-Agent\": CONFIG.get(\"user_agent\")}\n",
    "    resp = requests.get(url=url, params=params, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    resp_json = resp.json()\n",
    "    return resp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert camelCase to snake_case\n",
    "def convert(name):\n",
    "    regsub = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', regsub).lower()\n",
    "\n",
    "\n",
    "# Convert keys in json array\n",
    "def convert_array(arr):\n",
    "    new_arr = []\n",
    "    for i in arr:\n",
    "        if isinstance(i, list):\n",
    "            new_arr.append(convert_array(i))\n",
    "        elif isinstance(i, dict):\n",
    "            new_arr.append(convert_json(i))\n",
    "        else:\n",
    "            new_arr.append(i)\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "# Convert keys in json\n",
    "def convert_json(this_json):\n",
    "    out = {}\n",
    "    for key in this_json:\n",
    "        new_key = convert(key)\n",
    "        if isinstance(this_json[key], dict):\n",
    "            out[new_key] = convert_json(this_json[key])\n",
    "        elif isinstance(this_json[key], list):\n",
    "            out[new_key] = convert_array(this_json[key])\n",
    "        else:\n",
    "            out[new_key] = this_json[key]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attachments(this_json, path=None):\n",
    "    if path is None:\n",
    "        return this_json\n",
    "    i = 0\n",
    "    for record in this_json[path]:\n",
    "        if \"_embedded\" in record:\n",
    "            if \"attachments\" in record['_embedded']:\n",
    "                this_json[path][i]['attachments'] = this_json[path][i]['_embedded']['attachments']\n",
    "        i = i + 1\n",
    "    return this_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all _links and _embedded nodes from json\n",
    "def remove_embedded_links(this_json):\n",
    "    if not isinstance(this_json, (dict, list)):\n",
    "        return this_json\n",
    "    if isinstance(this_json, list):\n",
    "        return [remove_embedded_links(vv) for vv in this_json]\n",
    "    return {kk: remove_embedded_links(vv) for kk, vv in this_json.items()\n",
    "            if kk not in {'_embedded', '_links'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Authenticating and Get Request\n",
    "url = get_url('customers')\n",
    "params = {\n",
    "    \"sortField\": \"modifiedAt\", \n",
    "    \"sortOrder\": \"asc\",\n",
    "    \"modifiedSince\": \"2019-01-01T00:00:00Z\",\n",
    "    \"page\": 1\n",
    "}\n",
    "resp = request(url, params=params)\n",
    "items = convert_json(remove_embedded_links(move_attachments(resp['_embedded'])))\n",
    "pprint.pprint(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Sub-object Get Request\n",
    "base_obj = 'conversations'\n",
    "base_id = 868272638\n",
    "sub_obj = 'threads'\n",
    "url = get_url('{}/{}/{}'.format(base_obj, base_id, sub_obj))\n",
    "params = {\n",
    "    \"page\": 1\n",
    "}\n",
    "resp = request(url, params=params)\n",
    "new_resp = convert_json(remove_embedded_links(copy_attachments(resp['_embedded'], sub_obj)))\n",
    "pprint.pprint(new_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_path(path):\n",
    "    return os.path.join(str(os.path.dirname(os.path.realpath('__file__'))), str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schema(entity):\n",
    "    return utils.load_json(get_abs_path(\"schemas/{}.json\".format(entity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_write_schema(name, key_properties='id', bookmark_property='modified_at'):\n",
    "    schema = load_schema(name)\n",
    "    singer.write_schema(name, schema, key_properties, bookmark_properties=[bookmark_property])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start(key):\n",
    "    if key not in STATE:\n",
    "        STATE[key] = CONFIG['start_date']\n",
    "    return STATE[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(endpoint):\n",
    "    return BASE_API_URL + endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any date-times values can either be a string or a null.\n",
    "# If null, parsing the date results in an error.\n",
    "# Instead, removing the attribute before parsing ignores this error.\n",
    "def remove_empty_date_times(item, schema):\n",
    "    fields = []\n",
    "    for key in schema['properties']:\n",
    "        subschema = schema['properties'][key]\n",
    "        if subschema.get('format') == 'date-time':\n",
    "            fields.append(key)\n",
    "    for field in fields:\n",
    "        if item.get(field) is None:\n",
    "            try:\n",
    "                del item[field]\n",
    "            except KeyError as key_err:\n",
    "                LOGGER.info('Error: {}'.format(key_err))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_endpoint(schema_name, endpoint=None, path=None, with_updated_since=False, #pylint: disable=too-many-arguments\n",
    "                  bookmark_property=None, for_each_handler=None, map_handler=None): #pylint: disable=too-many-arguments\n",
    "    LOGGER.info(\"Syncing: {}\".format(schema_name))\n",
    "    schema = load_schema(schema_name)\n",
    "    singer.write_schema(schema_name,\n",
    "                        schema,\n",
    "                        [\"id\"],\n",
    "                        bookmark_properties=[bookmark_property])\n",
    "    start = get_start(schema_name)\n",
    "    start_dt = pendulum.parse(start)\n",
    "    updated_since = start_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    with Transformer() as transformer:\n",
    "        page = 1\n",
    "        total_pages = 1  # initial value, set with first API call\n",
    "        while page <= total_pages:\n",
    "            url = get_url(endpoint or schema_name)\n",
    "            LOGGER.info('URL: {}'.format(url))\n",
    "            if with_updated_since:\n",
    "                params = {\n",
    "                    \"sortField\": \"modifiedAt\",\n",
    "                    \"sortOrder\": \"asc\",\n",
    "                    \"modifiedSince\": updated_since\n",
    "                    }\n",
    "            else:\n",
    "                params = {}\n",
    "            params['page'] = page\n",
    "            response = request(url, params)\n",
    "            path = path or schema_name\n",
    "            data = {}\n",
    "            if '_embedded' in response:\n",
    "                data = convert_json(remove_embedded_links(\\\n",
    "                    copy_attachments(response['_embedded'], path)))[path]\n",
    "            for row in data:\n",
    "                if map_handler is not None:\n",
    "                    row = map_handler(row)\n",
    "                remove_empty_date_times(row, schema)\n",
    "                item = transformer.transform(row, schema)\n",
    "                if item.get(bookmark_property) is None or\\\n",
    "                    isinstance(item.get(bookmark_property), int):\n",
    "                    singer.write_record(schema_name, item)\n",
    "                    # take any additional actions required for the currently loaded endpoint\n",
    "                    if for_each_handler is not None:\n",
    "                        for_each_handler(row)\n",
    "                elif datetime.strptime(item[bookmark_property], \"%Y-%m-%dT%H:%M:%SZ\") >=\\\n",
    "                    datetime.strptime(start, \"%Y-%m-%dT%H:%M:%SZ\"):\n",
    "                    singer.write_record(schema_name, item)\n",
    "                    # take any additional actions required for the currently loaded endpoint\n",
    "                    if for_each_handler is not None:\n",
    "                        for_each_handler(row)\n",
    "                    utils.update_state(STATE, schema_name, item[bookmark_property])\n",
    "            page = response['page']['number']\n",
    "            total_pages = response['page']['totalPages']\n",
    "            LOGGER.info(\"Sync page {} of {}\".format(page, total_pages))\n",
    "            if page == 0 or page > 100:\n",
    "                break\n",
    "            page = page + 1\n",
    "    singer.write_state(STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_conversations():\n",
    "    def for_each_conversation(conversation):\n",
    "        def map_conversation_thread(thread):\n",
    "            thread['conversation_id'] = conversation['id']\n",
    "            return thread\n",
    "        # Sync conversation threads\n",
    "        sync_endpoint(\"conversation_threads\",\n",
    "                      endpoint=(\"conversations/{}/threads\".format(conversation['id'])),\n",
    "                      path=\"threads\",\n",
    "                      bookmark_property=\"created_at\",\n",
    "                      map_handler=map_conversation_thread)\n",
    "    sync_endpoint(\"conversations\",\n",
    "                  with_updated_since=True,\n",
    "                  bookmark_property=\"user_updated_at\",\n",
    "                  for_each_handler=for_each_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_mailboxes():\n",
    "    def for_each_mailbox(mailbox):\n",
    "        def map_mailbox_field(field):\n",
    "            field['mailbox_id'] = mailbox['id']\n",
    "            return field\n",
    "        def map_mailbox_folder(folder):\n",
    "            folder['mailbox_id'] = mailbox['id']\n",
    "            return folder\n",
    "        # Sync mailbox fields\n",
    "        sync_endpoint(\"mailbox_fields\",\n",
    "                      endpoint=(\"mailboxes/{}/fields\".format(mailbox['id'])),\n",
    "                      path=\"fields\",\n",
    "                      bookmark_property=\"id\",\n",
    "                      map_handler=map_mailbox_field)\n",
    "        # Sync mailbox folders\n",
    "        sync_endpoint(\"mailbox_folders\",\n",
    "                      endpoint=(\"mailboxes/{}/folders\".format(mailbox['id'])),\n",
    "                      path=\"folders\",\n",
    "                      bookmark_property=\"updated_at\",\n",
    "                      map_handler=map_mailbox_folder)\n",
    "    sync_endpoint(\"mailboxes\",\n",
    "                  bookmark_property=\"updated_at\",\n",
    "                  for_each_handler=for_each_mailbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: Test each object sync\n",
    "sync_mailboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sync():\n",
    "    LOGGER.info(\"Starting sync\")\n",
    "    # Sync objects\n",
    "    sync_endpoint(\"customers\",\n",
    "                  with_updated_since=True,\n",
    "                  bookmark_property=\"updated_at\")\n",
    "    sync_endpoint(\"users\",\n",
    "                  bookmark_property=\"updated_at\")\n",
    "    sync_endpoint(\"workflows\",\n",
    "                  bookmark_property=\"modified_at\")\n",
    "    sync_mailboxes()\n",
    "    sync_conversations()\n",
    "    LOGGER.info(\"Sync complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Authorization\n",
    "AUTH['access_token'], AUTH['expires_at'] = get_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING run complete sync\n",
    "do_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (helpscout)",
   "language": "python",
   "name": "helpscout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
